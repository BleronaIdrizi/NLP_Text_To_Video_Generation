{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe Dataset (over 2M) Food"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is a comprehensive collection of recipes from all around the world, ranging from simple dishes like bread to elaborate meals like Swedish midsummer smorgasbords. It is designed to facilitate projects that involve food analysis, recipe generation, or multimedia applications related to culinary arts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General-purpose libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Text-to-Video and NLP Libraries\n",
    "from transformers import pipeline\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# Video and Image Handling\n",
    "from moviepy import ImageSequenceClip, AudioFileClip\n",
    "import cv2  # OpenCV for image manipulation\n",
    "\n",
    "# Text-to-Speech\n",
    "from gtts import gTTS\n",
    "\n",
    "# Miscellaneous\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns = df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print dataset.\n",
    "def print_dataset(text, df):\n",
    "    print(\"\\n\" + text + \":\")\n",
    "    display(df.head())\n",
    "\n",
    "# Check for noisy data (e.g., special characters or unnecessary brackets)\n",
    "def find_noisy_data(column):\n",
    "    noisy_rows = df[column][df[column].str.contains(r\"[\\\\[\\\\]\\\\\\\\]|\\\\\\\"\")]\n",
    "    return noisy_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "df = pd.read_csv(\"../files/recipes_data.csv\")\n",
    "# df = pd.read_csv(\"../files/processed_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>directions</th>\n",
       "      <th>link</th>\n",
       "      <th>source</th>\n",
       "      <th>NER</th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No-Bake Nut Cookies</td>\n",
       "      <td>[\"1 c. firmly packed brown sugar\", \"1/2 c. eva...</td>\n",
       "      <td>[\"In a heavy 2-quart saucepan, mix brown sugar...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=44874</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[\"bite size shredded rice biscuits\", \"vanilla\"...</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jewell Ball'S Chicken</td>\n",
       "      <td>[\"1 small jar chipped beef, cut up\", \"4 boned ...</td>\n",
       "      <td>[\"Place chipped beef on bottom of baking dish....</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=699419</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[\"cream of mushroom soup\", \"beef\", \"sour cream...</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Creamy Corn</td>\n",
       "      <td>[\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg...</td>\n",
       "      <td>[\"In a slow cooker, combine all ingredients. C...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=10570</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[\"frozen corn\", \"pepper\", \"cream cheese\", \"gar...</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chicken Funny</td>\n",
       "      <td>[\"1 large whole chicken\", \"2 (10 1/2 oz.) cans...</td>\n",
       "      <td>[\"Boil and debone chicken.\", \"Put bite size pi...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=897570</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[\"chicken gravy\", \"cream of mushroom soup\", \"c...</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reeses Cups(Candy)</td>\n",
       "      <td>[\"1 c. peanut butter\", \"3/4 c. graham cracker ...</td>\n",
       "      <td>[\"Combine first four ingredients and press in ...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=659239</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[\"graham cracker crumbs\", \"powdered sugar\", \"p...</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title                                        ingredients  \\\n",
       "0    No-Bake Nut Cookies  [\"1 c. firmly packed brown sugar\", \"1/2 c. eva...   \n",
       "1  Jewell Ball'S Chicken  [\"1 small jar chipped beef, cut up\", \"4 boned ...   \n",
       "2            Creamy Corn  [\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg...   \n",
       "3          Chicken Funny  [\"1 large whole chicken\", \"2 (10 1/2 oz.) cans...   \n",
       "4   Reeses Cups(Candy)    [\"1 c. peanut butter\", \"3/4 c. graham cracker ...   \n",
       "\n",
       "                                          directions  \\\n",
       "0  [\"In a heavy 2-quart saucepan, mix brown sugar...   \n",
       "1  [\"Place chipped beef on bottom of baking dish....   \n",
       "2  [\"In a slow cooker, combine all ingredients. C...   \n",
       "3  [\"Boil and debone chicken.\", \"Put bite size pi...   \n",
       "4  [\"Combine first four ingredients and press in ...   \n",
       "\n",
       "                                              link    source  \\\n",
       "0   www.cookbooks.com/Recipe-Details.aspx?id=44874  Gathered   \n",
       "1  www.cookbooks.com/Recipe-Details.aspx?id=699419  Gathered   \n",
       "2   www.cookbooks.com/Recipe-Details.aspx?id=10570  Gathered   \n",
       "3  www.cookbooks.com/Recipe-Details.aspx?id=897570  Gathered   \n",
       "4  www.cookbooks.com/Recipe-Details.aspx?id=659239  Gathered   \n",
       "\n",
       "                                                 NER               site  \n",
       "0  [\"bite size shredded rice biscuits\", \"vanilla\"...  www.cookbooks.com  \n",
       "1  [\"cream of mushroom soup\", \"beef\", \"sour cream...  www.cookbooks.com  \n",
       "2  [\"frozen corn\", \"pepper\", \"cream cheese\", \"gar...  www.cookbooks.com  \n",
       "3  [\"chicken gravy\", \"cream of mushroom soup\", \"c...  www.cookbooks.com  \n",
       "4  [\"graham cracker crumbs\", \"powdered sugar\", \"p...  www.cookbooks.com  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_dataset(\"Dataset\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2231142 entries, 0 to 2231141\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Dtype \n",
      "---  ------       ----- \n",
      " 0   title        object\n",
      " 1   ingredients  object\n",
      " 2   directions   object\n",
      " 3   link         object\n",
      " 4   source       object\n",
      " 5   NER          object\n",
      " 6   site         object\n",
      "dtypes: object(7)\n",
      "memory usage: 119.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# To gain knowledge about data types, run this command:\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title          1\n",
       "ingredients    0\n",
       "directions     0\n",
       "link           0\n",
       "source         0\n",
       "NER            0\n",
       "site           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Command for checking for null values:\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Handling null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fshirja e rreshtave me vlera null në kolonat me vlera null\n",
    "df = df.dropna(subset=df_columns)\n",
    "#new_df.to_csv(\"../files/Preprocessed_Kosovo_News_Articles_Dataset.csv\", index=False)\n",
    "\n",
    "# Shfaqja e dataseti-it të modifikuar\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- Duplicate values in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Command to search duplicates\n",
    "print(\"Duplicates: \" + str(df.duplicated().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Duplicate values in title column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df[df['title'].duplicated(keep=False)]\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rows filter based on the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for rows where the title is \"Cherry Nut Bars\"\n",
    "cherry_nut_bars = df[df['title'] == \"Cherry Nut Bars\"]\n",
    "\n",
    "# Display the filtered rows\n",
    "# print_dataset(\"cherry_nut_bars\", cherry_nut_bars)\n",
    "print(cherry_nut_bars.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find duplicates in NER column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df[df['NER'].duplicated(keep=False)]\n",
    "print(duplicates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Removing nearly duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows before removing duplicates\n",
    "rows_before = len(df)\n",
    "\n",
    "# Identify duplicates based on 'title', 'NER', and 'ingredients'\n",
    "duplicates = df[df.duplicated(subset=['title', 'NER', 'ingredients'], keep=False)]\n",
    "\n",
    "# Log duplicate rows for verification\n",
    "print(\"Duplicate Rows:\")\n",
    "print(duplicates)\n",
    "\n",
    "# Remove duplicates, keeping only the first occurrence\n",
    "df_cleaned = df.drop_duplicates(subset=['title', 'NER'], keep='first')\n",
    "\n",
    "# Number of rows after removing duplicates\n",
    "rows_after = len(df_cleaned)\n",
    "\n",
    "# Calculate the number of deleted rows\n",
    "deleted_rows = rows_before - rows_after\n",
    "\n",
    "df = df_cleaned\n",
    "\n",
    "# Logs\n",
    "print(f\"\\nRows before removing duplicates: {rows_before}\")\n",
    "print(f\"Rows after removing duplicates: {rows_after}\")\n",
    "print(f\"Number of rows deleted: {deleted_rows}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rows after removing nearly duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for rows where the title is \"Cherry Nut Bars\"\n",
    "cherry_nut_bars = df[df['title'] == \"Cherry Nut Bars\"]\n",
    "\n",
    "# Display the filtered rows\n",
    "# print_dataset(\"cherry_nut_bars\", cherry_nut_bars)\n",
    "print(cherry_nut_bars.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each column in the DataFrame\n",
    "for column in df_columns:\n",
    "    nan_count = df[column].isna().sum()  # Count missing (NaN) values in the column\n",
    "    print(f\"The number of missing values detected in the column '{column}' is: {nan_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Handling NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete columns 'City' and 'Salary'\n",
    "columns_to_delete = ['link', 'source', 'site']\n",
    "df.drop(columns=columns_to_delete, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify titles with special characters\n",
    "print(\"Titles with special characters:\")\n",
    "print(df[df['title'].str.contains(r'[^\\w\\s]', regex=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Handling special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters from titles\n",
    "df['title'] = df['title'].str.replace(r'[^\\w\\s]', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display noisy data in the 'ingredients' column\n",
    "noisy_data = find_noisy_data('ingredients')\n",
    "print(\"Noisy Data in 'ingredients':\")\n",
    "print(noisy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display noisy data in the 'directions' column\n",
    "noisy_data = find_noisy_data('directions')\n",
    "print(\"Noisy Data in 'directions':\")\n",
    "print(noisy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Noisy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean noisy data in the 'ingredients' column\n",
    "df['ingredients'] = df['ingredients'] \\\n",
    "    .str.replace(r'\\\\\"', '\"', regex=True) \\\n",
    "    .str.replace(r'[\\[\\]]', '', regex=True) \\\n",
    "    .str.replace(r'\\\\', '', regex=True) \\\n",
    "    .str.strip()\n",
    "\n",
    "# Verify the cleaned column\n",
    "print(\"Cleaned Ingredients Column:\")\n",
    "print(df['ingredients'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean noisy data in the 'ingredients' column\n",
    "df['directions'] = df['directions'] \\\n",
    "    .str.replace(r'\\\\\"', '\"', regex=True) \\\n",
    "    .str.replace(r'[\\[\\]]', '', regex=True) \\\n",
    "    .str.replace(r'\\\\', '', regex=True) \\\n",
    "    .str.strip()\n",
    "\n",
    "# Verify the cleaned column\n",
    "print(\"Cleaned Ingredients Column:\")\n",
    "print(df['directions'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of each unique string\n",
    "frequency_counts = df['title'].value_counts()\n",
    "\n",
    "# Define a threshold for rare occurrences (e.g., frequency = 1)\n",
    "outliers = frequency_counts[frequency_counts == 1]\n",
    "\n",
    "# Display outliers\n",
    "print(\"Outliers based on frequency:\")\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find groups data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Krijimi i një TF-IDF vektorizuesi\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df['title'])\n",
    "\n",
    "# Klasterizimi me K-Means\n",
    "num_clusters = 3  # Numri i grupeve\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Shtimi i grupeve për secilin titull\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "# Shfaqja e rezultateve\n",
    "for title, cluster in zip(df['title'], clusters):\n",
    "    print(f\"'{title}' është në grupin {cluster}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clustering recipe titles by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "# Get 50% of the data\n",
    "df = df.sample(frac=0.5, random_state=42)\n",
    "\n",
    "# Reset the index to align with PCA results\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Handle missing values\n",
    "df['title'] = df['title'].fillna('')\n",
    "df['ingredients'] = df['ingredients'].fillna('')\n",
    "\n",
    "# TF-IDF Vectorization on Titles\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "X = vectorizer.fit_transform(df['title'])\n",
    "\n",
    "print(f\"TF-IDF Matrix Shape: {X.shape}\")\n",
    "\n",
    "# K-Means Clustering\n",
    "num_clusters = 3\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Assign Cluster Labels\n",
    "clusters = kmeans.labels_\n",
    "df['Cluster'] = clusters\n",
    "\n",
    "# Manually Define Categories for Clusters\n",
    "cluster_names = {\n",
    "    0: 'Dessert Recipes',\n",
    "    1: 'Dinner Recipes',\n",
    "    2: 'Breakfast Recipes'\n",
    "}\n",
    "\n",
    "# Assign Category Names\n",
    "df['Category'] = df['Cluster'].map(cluster_names)\n",
    "\n",
    "# Print Grouped Titles by Cluster\n",
    "print(\"\\n--- Titles Grouped by Cluster ---\")\n",
    "for cluster, category in cluster_names.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    titles_in_group = df[df['Cluster'] == cluster]['title'].tolist()\n",
    "    for title in titles_in_group:\n",
    "        print(f\"- {title}\")\n",
    "\n",
    "# Dimensionality Reduction with PCA for Visualization\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_reduced = pca.fit_transform(X.toarray())\n",
    "print(f\"PCA Reduced Shape: {X_reduced.shape}\")\n",
    "\n",
    "# Visualization with Category Names\n",
    "plt.figure(figsize=(10, 7))\n",
    "for cluster, category in cluster_names.items():\n",
    "    points = X_reduced[df['Cluster'] == cluster]\n",
    "    plt.scatter(points[:, 0], points[:, 1], label=category)\n",
    "\n",
    "# Removed Add Titles as Labels (plt.text)\n",
    "# for i, (x, y) in enumerate(X_reduced):\n",
    "#     plt.text(x, y, df.iloc[i]['title'], fontsize=8)\n",
    "\n",
    "plt.title(\"Clustering of Recipe Titles by Category\")\n",
    "plt.xlabel(\"Title\")\n",
    "plt.ylabel(\"Category\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Video Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import os\n",
    "\n",
    "# Load Stable Diffusion model\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
    "pipe = pipe.to(\"cpu\")  # Use CPU instead of GPU\n",
    "\n",
    "# Create a directory for generated images\n",
    "os.makedirs(\"generated_frames\", exist_ok=True)\n",
    "\n",
    "print(\"Stable Diffusion model loaded successfully in CPU mode!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Images for Recipe Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "sample_df = sample_df.reset_index(drop=True)\n",
    "\n",
    "# Select the first recipe\n",
    "recipe_title = sample_df['title'][0]\n",
    "recipe_steps = sample_df['directions'][0]  # This is already a list\n",
    "\n",
    "frames = []  # To store generated image paths\n",
    "for i, step in enumerate(recipe_steps):\n",
    "    prompt = f\"Artistic representation of: {step}\"\n",
    "    image = pipe(prompt).images[0]\n",
    "    frame_path = f\"generated_frames/{recipe_title.replace(' ', '_')}_step_{i+1}.png\"\n",
    "    image.save(frame_path)\n",
    "    frames.append(frame_path)\n",
    "\n",
    "print(f\"Generated {len(frames)} images for '{recipe_title}' recipe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Frames into a Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imageio_ffmpeg\n",
    "\n",
    "ffmpeg_path = imageio_ffmpeg.get_ffmpeg_exe()\n",
    "print(\"FFmpeg binary path:\", ffmpeg_path)\n",
    "\n",
    "# Explicitly set ffmpeg path\n",
    "os.environ[\"IMAGEIO_FFMPEG_EXE\"] = \"/opt/homebrew/bin/ffmpeg\"\n",
    "\n",
    "# Verify the ffmpeg path\n",
    "print(\"Using FFmpeg binary at:\", imageio_ffmpeg.get_ffmpeg_exe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing frames\n",
    "frame_directory = \"generated_frames\"\n",
    "\n",
    "# Get all image files in the directory\n",
    "all_frames = sorted([f for f in os.listdir(frame_directory) if f.endswith(\".png\")])\n",
    "\n",
    "# Group frames by their prefix\n",
    "frame_groups = {}\n",
    "for frame in all_frames:\n",
    "    prefix = \"_\".join(frame.split(\"_\")[:-2])  # Extract prefix (everything except step_x)\n",
    "    if prefix not in frame_groups:\n",
    "        frame_groups[prefix] = []\n",
    "    frame_groups[prefix].append(os.path.join(frame_directory, frame))\n",
    "\n",
    "# Create a video for each group\n",
    "fps = 1  # Frames per second\n",
    "output_directory = \"../videos/\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "for prefix, frames in frame_groups.items():\n",
    "    video_path = os.path.join(output_directory, f\"{prefix}.mp4\")\n",
    "    clip = ImageSequenceClip(frames, fps=fps)\n",
    "    clip.write_videofile(video_path, codec=\"libx264\")\n",
    "    print(f\"Video saved: {video_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentation in NLP involves breaking down a larger piece of text into smaller, meaningful units such as sentences or paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the type of the `directions` column\n",
    "print(type(df['directions'][0]))\n",
    "\n",
    "# If `directions` is already a list, skip the parsing step\n",
    "# Flatten and segment the directions into smaller pieces\n",
    "segmented_directions = []\n",
    "for index, row in df.iterrows():\n",
    "    for step in row['directions']:\n",
    "        # Split steps into smaller segments using `.split('. ')`\n",
    "        segments = step.split('. ')\n",
    "        for segment in segments:\n",
    "            # Avoid adding empty strings\n",
    "            if segment.strip():\n",
    "                segmented_directions.append({\n",
    "                    \"title\": row[\"title\"],\n",
    "                    \"direction_segment\": segment.strip()  # Clean whitespace\n",
    "                })\n",
    "\n",
    "# Convert the segmented directions into a new DataFrame\n",
    "segmented_df = pd.DataFrame(segmented_directions)\n",
    "\n",
    "# Export the segmented DataFrame to a CSV file\n",
    "segmented_df.to_csv(\"../files/1_segmentation.csv\", index=False)\n",
    "\n",
    "# Print the first few rows for verification\n",
    "print(segmented_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is used in MLP to split paragraphs and sentences into smaller units that can be more easily assigned meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Check if `directions` is a list or a string\n",
    "if isinstance(df['directions'][0], str):\n",
    "    import ast\n",
    "    df['directions'] = df['directions'].apply(ast.literal_eval)\n",
    "\n",
    "# Segment and tokenize the directions\n",
    "segmented_tokenized_directions = []\n",
    "for index, row in df.iterrows():\n",
    "    for step in row['directions']:\n",
    "        # Segment the step into smaller parts\n",
    "        segments = step.split('. ')\n",
    "        for segment in segments:\n",
    "            # Clean whitespace and ensure the segment is not empty\n",
    "            clean_segment = segment.strip()\n",
    "            if clean_segment:\n",
    "                try:\n",
    "                    # Tokenize the segment into words\n",
    "                    tokens = word_tokenize(clean_segment)\n",
    "                    segmented_tokenized_directions.append({\n",
    "                        \"title\": row[\"title\"],\n",
    "                        \"direction_segment\": clean_segment,\n",
    "                        \"tokens\": tokens\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error tokenizing: {clean_segment}. Error: {e}\")\n",
    "\n",
    "# Convert the results into a DataFrame\n",
    "segmented_tokenized_df = pd.DataFrame(segmented_tokenized_directions)\n",
    "\n",
    "# Convert the tokens column to a string for proper CSV export\n",
    "segmented_tokenized_df['tokens'] = segmented_tokenized_df['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Export the segmented and tokenized DataFrame to a CSV file\n",
    "segmented_tokenized_df.to_csv(\"../files/2_tokenization.csv\", index=False)\n",
    "\n",
    "# Print the first few rows for verification\n",
    "print(segmented_tokenized_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
