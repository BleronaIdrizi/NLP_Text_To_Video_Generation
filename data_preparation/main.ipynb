{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe Dataset (over 2M) Food"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is a comprehensive collection of recipes from all around the world, ranging from simple dishes like bread to elaborate meals like Swedish midsummer smorgasbords. It is designed to facilitate projects that involve food analysis, recipe generation, or multimedia applications related to culinary arts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General-purpose libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Text-to-Video and NLP Libraries\n",
    "from transformers import pipeline\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# Video and Image Handling\n",
    "from moviepy import ImageSequenceClip, AudioFileClip\n",
    "import cv2  # OpenCV for image manipulation\n",
    "\n",
    "# Text-to-Speech\n",
    "from gtts import gTTS\n",
    "\n",
    "# Miscellaneous\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns = df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print dataset.\n",
    "def print_dataset(text, df):\n",
    "    print(\"\\n\" + text + \":\")\n",
    "    display(df.head())\n",
    "\n",
    "# Check for noisy data (e.g., special characters or unnecessary brackets)\n",
    "def find_noisy_data(column):\n",
    "    noisy_rows = df[column][df[column].str.contains(r\"[\\\\[\\\\]\\\\\\\\]|\\\\\\\"\")]\n",
    "    return noisy_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "df = pd.read_csv(\"../files/recipes_data.csv\")\n",
    "# df = pd.read_csv(\"../files/processed_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>directions</th>\n",
       "      <th>link</th>\n",
       "      <th>source</th>\n",
       "      <th>NER</th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No-Bake Nut Cookies</td>\n",
       "      <td>[\"1 c. firmly packed brown sugar\", \"1/2 c. eva...</td>\n",
       "      <td>[\"In a heavy 2-quart saucepan, mix brown sugar...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=44874</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[\"bite size shredded rice biscuits\", \"vanilla\"...</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jewell Ball'S Chicken</td>\n",
       "      <td>[\"1 small jar chipped beef, cut up\", \"4 boned ...</td>\n",
       "      <td>[\"Place chipped beef on bottom of baking dish....</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=699419</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[\"cream of mushroom soup\", \"beef\", \"sour cream...</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Creamy Corn</td>\n",
       "      <td>[\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg...</td>\n",
       "      <td>[\"In a slow cooker, combine all ingredients. C...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=10570</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[\"frozen corn\", \"pepper\", \"cream cheese\", \"gar...</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chicken Funny</td>\n",
       "      <td>[\"1 large whole chicken\", \"2 (10 1/2 oz.) cans...</td>\n",
       "      <td>[\"Boil and debone chicken.\", \"Put bite size pi...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=897570</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[\"chicken gravy\", \"cream of mushroom soup\", \"c...</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reeses Cups(Candy)</td>\n",
       "      <td>[\"1 c. peanut butter\", \"3/4 c. graham cracker ...</td>\n",
       "      <td>[\"Combine first four ingredients and press in ...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=659239</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[\"graham cracker crumbs\", \"powdered sugar\", \"p...</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title                                        ingredients  \\\n",
       "0    No-Bake Nut Cookies  [\"1 c. firmly packed brown sugar\", \"1/2 c. eva...   \n",
       "1  Jewell Ball'S Chicken  [\"1 small jar chipped beef, cut up\", \"4 boned ...   \n",
       "2            Creamy Corn  [\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg...   \n",
       "3          Chicken Funny  [\"1 large whole chicken\", \"2 (10 1/2 oz.) cans...   \n",
       "4   Reeses Cups(Candy)    [\"1 c. peanut butter\", \"3/4 c. graham cracker ...   \n",
       "\n",
       "                                          directions  \\\n",
       "0  [\"In a heavy 2-quart saucepan, mix brown sugar...   \n",
       "1  [\"Place chipped beef on bottom of baking dish....   \n",
       "2  [\"In a slow cooker, combine all ingredients. C...   \n",
       "3  [\"Boil and debone chicken.\", \"Put bite size pi...   \n",
       "4  [\"Combine first four ingredients and press in ...   \n",
       "\n",
       "                                              link    source  \\\n",
       "0   www.cookbooks.com/Recipe-Details.aspx?id=44874  Gathered   \n",
       "1  www.cookbooks.com/Recipe-Details.aspx?id=699419  Gathered   \n",
       "2   www.cookbooks.com/Recipe-Details.aspx?id=10570  Gathered   \n",
       "3  www.cookbooks.com/Recipe-Details.aspx?id=897570  Gathered   \n",
       "4  www.cookbooks.com/Recipe-Details.aspx?id=659239  Gathered   \n",
       "\n",
       "                                                 NER               site  \n",
       "0  [\"bite size shredded rice biscuits\", \"vanilla\"...  www.cookbooks.com  \n",
       "1  [\"cream of mushroom soup\", \"beef\", \"sour cream...  www.cookbooks.com  \n",
       "2  [\"frozen corn\", \"pepper\", \"cream cheese\", \"gar...  www.cookbooks.com  \n",
       "3  [\"chicken gravy\", \"cream of mushroom soup\", \"c...  www.cookbooks.com  \n",
       "4  [\"graham cracker crumbs\", \"powdered sugar\", \"p...  www.cookbooks.com  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_dataset(\"Dataset\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2231142 entries, 0 to 2231141\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Dtype \n",
      "---  ------       ----- \n",
      " 0   title        object\n",
      " 1   ingredients  object\n",
      " 2   directions   object\n",
      " 3   link         object\n",
      " 4   source       object\n",
      " 5   NER          object\n",
      " 6   site         object\n",
      "dtypes: object(7)\n",
      "memory usage: 119.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# To gain knowledge about data types, run this command:\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title          1\n",
       "ingredients    0\n",
       "directions     0\n",
       "link           0\n",
       "source         0\n",
       "NER            0\n",
       "site           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Command for checking for null values:\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Handling null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fshirja e rreshtave me vlera null në kolonat me vlera null\n",
    "df = df.dropna(subset=df_columns)\n",
    "#new_df.to_csv(\"../files/Preprocessed_Kosovo_News_Articles_Dataset.csv\", index=False)\n",
    "\n",
    "# Shfaqja e dataseti-it të modifikuar\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Duplicate values in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command to search duplicates\n",
    "print(\"Duplicates: \" + str(df.duplicated().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Duplicate values in title column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df[df['title'].duplicated(keep=False)]\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rows filter based on the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for rows where the title is \"Cherry Nut Bars\"\n",
    "cherry_nut_bars = df[df['title'] == \"Cherry Nut Bars\"]\n",
    "\n",
    "# Display the filtered rows\n",
    "# print_dataset(\"cherry_nut_bars\", cherry_nut_bars)\n",
    "print(cherry_nut_bars.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find duplicates in NER column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df[df['NER'].duplicated(keep=False)]\n",
    "print(duplicates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Removing nearly duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows before removing duplicates\n",
    "rows_before = len(df)\n",
    "\n",
    "# Identify duplicates based on 'title', 'NER', and 'ingredients'\n",
    "duplicates = df[df.duplicated(subset=['title', 'NER', 'ingredients'], keep=False)]\n",
    "\n",
    "# Log duplicate rows for verification\n",
    "print(\"Duplicate Rows:\")\n",
    "print(duplicates)\n",
    "\n",
    "# Remove duplicates, keeping only the first occurrence\n",
    "df_cleaned = df.drop_duplicates(subset=['title', 'NER'], keep='first')\n",
    "\n",
    "# Number of rows after removing duplicates\n",
    "rows_after = len(df_cleaned)\n",
    "\n",
    "# Calculate the number of deleted rows\n",
    "deleted_rows = rows_before - rows_after\n",
    "\n",
    "df = df_cleaned\n",
    "\n",
    "# Logs\n",
    "print(f\"\\nRows before removing duplicates: {rows_before}\")\n",
    "print(f\"Rows after removing duplicates: {rows_after}\")\n",
    "print(f\"Number of rows deleted: {deleted_rows}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rows after removing nearly duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for rows where the title is \"Cherry Nut Bars\"\n",
    "cherry_nut_bars = df[df['title'] == \"Cherry Nut Bars\"]\n",
    "\n",
    "# Display the filtered rows\n",
    "# print_dataset(\"cherry_nut_bars\", cherry_nut_bars)\n",
    "print(cherry_nut_bars.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each column in the DataFrame\n",
    "for column in df_columns:\n",
    "    nan_count = df[column].isna().sum()  # Count missing (NaN) values in the column\n",
    "    print(f\"The number of missing values detected in the column '{column}' is: {nan_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Handling NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete columns 'City' and 'Salary'\n",
    "columns_to_delete = ['link', 'source', 'site']\n",
    "df.drop(columns=columns_to_delete, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify titles with special characters\n",
    "print(\"Titles with special characters:\")\n",
    "print(df[df['title'].str.contains(r'[^\\w\\s]', regex=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Handling special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters from titles\n",
    "df['title'] = df['title'].str.replace(r'[^\\w\\s]', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display noisy data in the 'ingredients' column\n",
    "noisy_data = find_noisy_data('ingredients')\n",
    "print(\"Noisy Data in 'ingredients':\")\n",
    "print(noisy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display noisy data in the 'directions' column\n",
    "noisy_data = find_noisy_data('directions')\n",
    "print(\"Noisy Data in 'directions':\")\n",
    "print(noisy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Noisy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean noisy data in the 'ingredients' column\n",
    "df['ingredients'] = df['ingredients'] \\\n",
    "    .str.replace(r'\\\\\"', '\"', regex=True) \\\n",
    "    .str.replace(r'[\\[\\]]', '', regex=True) \\\n",
    "    .str.replace(r'\\\\', '', regex=True) \\\n",
    "    .str.strip()\n",
    "\n",
    "# Verify the cleaned column\n",
    "print(\"Cleaned Ingredients Column:\")\n",
    "print(df['ingredients'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean noisy data in the 'ingredients' column\n",
    "df['directions'] = df['directions'] \\\n",
    "    .str.replace(r'\\\\\"', '\"', regex=True) \\\n",
    "    .str.replace(r'[\\[\\]]', '', regex=True) \\\n",
    "    .str.replace(r'\\\\', '', regex=True) \\\n",
    "    .str.strip()\n",
    "\n",
    "# Verify the cleaned column\n",
    "print(\"Cleaned Ingredients Column:\")\n",
    "print(df['directions'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of each unique string\n",
    "frequency_counts = df['title'].value_counts()\n",
    "\n",
    "# Define a threshold for rare occurrences (e.g., frequency = 1)\n",
    "outliers = frequency_counts[frequency_counts == 1]\n",
    "\n",
    "# Display outliers\n",
    "print(\"Outliers based on frequency:\")\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find groups data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Krijimi i një TF-IDF vektorizuesi\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df['title'])\n",
    "\n",
    "# Klasterizimi me K-Means\n",
    "num_clusters = 3  # Numri i grupeve\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Shtimi i grupeve për secilin titull\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "# Shfaqja e rezultateve\n",
    "for title, cluster in zip(df['title'], clusters):\n",
    "    print(f\"'{title}' është në grupin {cluster}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clustering recipe titles by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "# Get 50% of the data\n",
    "df = df.sample(frac=0.5, random_state=42)\n",
    "\n",
    "# Reset the index to align with PCA results\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Handle missing values\n",
    "df['title'] = df['title'].fillna('')\n",
    "df['ingredients'] = df['ingredients'].fillna('')\n",
    "\n",
    "# TF-IDF Vectorization on Titles\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "X = vectorizer.fit_transform(df['title'])\n",
    "\n",
    "print(f\"TF-IDF Matrix Shape: {X.shape}\")\n",
    "\n",
    "# K-Means Clustering\n",
    "num_clusters = 3\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Assign Cluster Labels\n",
    "clusters = kmeans.labels_\n",
    "df['Cluster'] = clusters\n",
    "\n",
    "# Manually Define Categories for Clusters\n",
    "cluster_names = {\n",
    "    0: 'Dessert Recipes',\n",
    "    1: 'Dinner Recipes',\n",
    "    2: 'Breakfast Recipes'\n",
    "}\n",
    "\n",
    "# Assign Category Names\n",
    "df['Category'] = df['Cluster'].map(cluster_names)\n",
    "\n",
    "# Print Grouped Titles by Cluster\n",
    "print(\"\\n--- Titles Grouped by Cluster ---\")\n",
    "for cluster, category in cluster_names.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    titles_in_group = df[df['Cluster'] == cluster]['title'].tolist()\n",
    "    for title in titles_in_group:\n",
    "        print(f\"- {title}\")\n",
    "\n",
    "# Dimensionality Reduction with PCA for Visualization\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_reduced = pca.fit_transform(X.toarray())\n",
    "print(f\"PCA Reduced Shape: {X_reduced.shape}\")\n",
    "\n",
    "# Visualization with Category Names\n",
    "plt.figure(figsize=(10, 7))\n",
    "for cluster, category in cluster_names.items():\n",
    "    points = X_reduced[df['Cluster'] == cluster]\n",
    "    plt.scatter(points[:, 0], points[:, 1], label=category)\n",
    "\n",
    "# Removed Add Titles as Labels (plt.text)\n",
    "# for i, (x, y) in enumerate(X_reduced):\n",
    "#     plt.text(x, y, df.iloc[i]['title'], fontsize=8)\n",
    "\n",
    "plt.title(\"Clustering of Recipe Titles by Category\")\n",
    "plt.xlabel(\"Title\")\n",
    "plt.ylabel(\"Category\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Video Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import os\n",
    "\n",
    "# Load Stable Diffusion model\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
    "pipe = pipe.to(\"cpu\")  # Use CPU instead of GPU\n",
    "\n",
    "# Create a directory for generated images\n",
    "os.makedirs(\"generated_frames\", exist_ok=True)\n",
    "\n",
    "print(\"Stable Diffusion model loaded successfully in CPU mode!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Images for Recipe Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "sample_df = sample_df.reset_index(drop=True)\n",
    "\n",
    "# Select the first recipe\n",
    "recipe_title = sample_df['title'][0]\n",
    "recipe_steps = sample_df['directions'][0]  # This is already a list\n",
    "\n",
    "frames = []  # To store generated image paths\n",
    "for i, step in enumerate(recipe_steps):\n",
    "    prompt = f\"Artistic representation of: {step}\"\n",
    "    image = pipe(prompt).images[0]\n",
    "    frame_path = f\"generated_frames/{recipe_title.replace(' ', '_')}_step_{i+1}.png\"\n",
    "    image.save(frame_path)\n",
    "    frames.append(frame_path)\n",
    "\n",
    "print(f\"Generated {len(frames)} images for '{recipe_title}' recipe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Frames into a Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imageio_ffmpeg\n",
    "\n",
    "ffmpeg_path = imageio_ffmpeg.get_ffmpeg_exe()\n",
    "print(\"FFmpeg binary path:\", ffmpeg_path)\n",
    "\n",
    "# Explicitly set ffmpeg path\n",
    "os.environ[\"IMAGEIO_FFMPEG_EXE\"] = \"/opt/homebrew/bin/ffmpeg\"\n",
    "\n",
    "# Verify the ffmpeg path\n",
    "print(\"Using FFmpeg binary at:\", imageio_ffmpeg.get_ffmpeg_exe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing frames\n",
    "frame_directory = \"generated_frames\"\n",
    "\n",
    "# Get all image files in the directory\n",
    "all_frames = sorted([f for f in os.listdir(frame_directory) if f.endswith(\".png\")])\n",
    "\n",
    "# Group frames by their prefix\n",
    "frame_groups = {}\n",
    "for frame in all_frames:\n",
    "    prefix = \"_\".join(frame.split(\"_\")[:-2])  # Extract prefix (everything except step_x)\n",
    "    if prefix not in frame_groups:\n",
    "        frame_groups[prefix] = []\n",
    "    frame_groups[prefix].append(os.path.join(frame_directory, frame))\n",
    "\n",
    "# Create a video for each group\n",
    "fps = 1  # Frames per second\n",
    "output_directory = \"../videos/\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "for prefix, frames in frame_groups.items():\n",
    "    video_path = os.path.join(output_directory, f\"{prefix}.mp4\")\n",
    "    clip = ImageSequenceClip(frames, fps=fps)\n",
    "    clip.write_videofile(video_path, codec=\"libx264\")\n",
    "    print(f\"Video saved: {video_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentation in NLP involves breaking down a larger piece of text into smaller, meaningful units such as sentences or paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the type of the `directions` column\n",
    "print(type(df['directions'][0]))\n",
    "\n",
    "# If `directions` is already a list, skip the parsing step\n",
    "# Flatten and segment the directions into smaller pieces\n",
    "segmented_directions = []\n",
    "for index, row in df.iterrows():\n",
    "    for step in row['directions']:\n",
    "        # Split steps into smaller segments using `.split('. ')`\n",
    "        segments = step.split('. ')\n",
    "        for segment in segments:\n",
    "            # Avoid adding empty strings\n",
    "            if segment.strip():\n",
    "                segmented_directions.append({\n",
    "                    \"title\": row[\"title\"],\n",
    "                    \"direction_segment\": segment.strip()  # Clean whitespace\n",
    "                })\n",
    "\n",
    "# Convert the segmented directions into a new DataFrame\n",
    "segmented_df = pd.DataFrame(segmented_directions)\n",
    "\n",
    "# Export the segmented DataFrame to a CSV file\n",
    "segmented_df.to_csv(\"../files/1_segmentation.csv\", index=False)\n",
    "\n",
    "# Print the first few rows for verification\n",
    "print(segmented_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is used in MLP to split paragraphs and sentences into smaller units that can be more easily assigned meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Check if `directions` is a list or a string\n",
    "if isinstance(df['directions'][0], str):\n",
    "    import ast\n",
    "    df['directions'] = df['directions'].apply(ast.literal_eval)\n",
    "\n",
    "# Segment and tokenize the directions\n",
    "segmented_tokenized_directions = []\n",
    "for index, row in df.iterrows():\n",
    "    for step in row['directions']:\n",
    "        # Segment the step into smaller parts\n",
    "        segments = step.split('. ')\n",
    "        for segment in segments:\n",
    "            # Clean whitespace and ensure the segment is not empty\n",
    "            clean_segment = segment.strip()\n",
    "            if clean_segment:\n",
    "                try:\n",
    "                    # Tokenize the segment into words\n",
    "                    tokens = word_tokenize(clean_segment)\n",
    "                    segmented_tokenized_directions.append({\n",
    "                        \"title\": row[\"title\"],\n",
    "                        \"direction_segment\": clean_segment,\n",
    "                        \"tokens\": tokens\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error tokenizing: {clean_segment}. Error: {e}\")\n",
    "\n",
    "# Convert the results into a DataFrame\n",
    "segmented_tokenized_df = pd.DataFrame(segmented_tokenized_directions)\n",
    "\n",
    "# Convert the tokens column to a string for proper CSV export\n",
    "segmented_tokenized_df['tokens'] = segmented_tokenized_df['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Export the segmented and tokenized DataFrame to a CSV file\n",
    "segmented_tokenized_df.to_csv(\"../files/2_tokenization.csv\", index=False)\n",
    "\n",
    "# Print the first few rows for verification\n",
    "print(segmented_tokenized_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are a set of commonly used words in a language. Examples of stop words in English are “a,” “the,” “is,” “are,” etc. Stop words are commonly used in Text Mining and Natural Language Processing (NLP) to eliminate words that are so widely used that they carry very little useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')       # Tokenizer\n",
    "nltk.download('stopwords')   # Stop words list\n",
    "\n",
    "# Ensure `directions` is a list\n",
    "if isinstance(df['directions'][0], str):\n",
    "    import ast\n",
    "    df['directions'] = df['directions'].apply(ast.literal_eval)\n",
    "\n",
    "# Get the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Segment, tokenize, and remove stop words from directions\n",
    "segmented_tokenized_directions = []\n",
    "for index, row in df.iterrows():\n",
    "    for step in row['directions']:\n",
    "        # Segment the step into smaller parts\n",
    "        segments = step.split('. ')\n",
    "        for segment in segments:\n",
    "            # Clean whitespace and ensure the segment is not empty\n",
    "            clean_segment = segment.strip()\n",
    "            if clean_segment:\n",
    "                try:\n",
    "                    # Tokenize the segment into words\n",
    "                    tokens = word_tokenize(clean_segment)\n",
    "                    \n",
    "                    # Remove stop words from the token list\n",
    "                    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "                    \n",
    "                    segmented_tokenized_directions.append({\n",
    "                        \"title\": row[\"title\"],\n",
    "                        \"direction_segment\": clean_segment,\n",
    "                        \"tokens\": filtered_tokens\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error tokenizing: {clean_segment}. Error: {e}\")\n",
    "\n",
    "# Convert the results into a DataFrame\n",
    "segmented_tokenized_df = pd.DataFrame(segmented_tokenized_directions)\n",
    "\n",
    "# Convert the tokens column to a string for proper CSV export\n",
    "segmented_tokenized_df['tokens'] = segmented_tokenized_df['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Export the segmented and tokenized DataFrame to a CSV file\n",
    "segmented_tokenized_df.to_csv(\"../files/3_stopwords.csv\", index=False)\n",
    "\n",
    "# Print the first few rows for verification\n",
    "print(segmented_tokenized_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is a text preprocessing technique in natural language processing (NLP). Specifically, it is the process of reducing inflected form of a word to one so-called “stem,” or root form, also known as a “lemma” in linguistics.N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')       # Tokenizer\n",
    "nltk.download('stopwords')   # Stop words list\n",
    "\n",
    "# Initialize the PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Ensure `directions` is a list\n",
    "if isinstance(df['directions'][0], str):\n",
    "    import ast\n",
    "    df['directions'] = df['directions'].apply(ast.literal_eval)\n",
    "\n",
    "# Get the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Segment, tokenize, remove stop words, and apply stemming\n",
    "segmented_tokenized_directions = []\n",
    "for index, row in df.iterrows():\n",
    "    for step in row['directions']:\n",
    "        # Segment the step into smaller parts\n",
    "        segments = step.split('. ')\n",
    "        for segment in segments:\n",
    "            # Clean whitespace and ensure the segment is not empty\n",
    "            clean_segment = segment.strip()\n",
    "            if clean_segment:\n",
    "                try:\n",
    "                    # Tokenize the segment into words\n",
    "                    tokens = word_tokenize(clean_segment)\n",
    "                    \n",
    "                    # Remove stop words from the token list\n",
    "                    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "                    \n",
    "                    # Apply stemming to the filtered tokens\n",
    "                    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "                    \n",
    "                    segmented_tokenized_directions.append({\n",
    "                        \"title\": row[\"title\"],\n",
    "                        \"direction_segment\": clean_segment,\n",
    "                        \"tokens\": stemmed_tokens\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error tokenizing: {clean_segment}. Error: {e}\")\n",
    "\n",
    "# Convert the results into a DataFrame\n",
    "segmented_tokenized_df = pd.DataFrame(segmented_tokenized_directions)\n",
    "\n",
    "# Convert the tokens column to a string for proper CSV export\n",
    "segmented_tokenized_df['tokens'] = segmented_tokenized_df['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Export the segmented and tokenized DataFrame to a CSV file\n",
    "segmented_tokenized_df.to_csv(\"../files/4_stemming.csv\", index=False)\n",
    "\n",
    "# Print the first few rows for verification\n",
    "print(segmented_tokenized_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is a text pre-processing technique used in natural language processing (NLP) models to break a word down to its root meaning to identify similarities. For example, a lemmatization algorithm would reduce the word better to its root word, or lemme, good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')       # Tokenizer\n",
    "nltk.download('stopwords')   # Stop words list\n",
    "nltk.download('wordnet')     # WordNet for lemmatization\n",
    "nltk.download('omw-1.4')     # WordNet lemmatizer's dependency\n",
    "\n",
    "# Initialize the WordNetLemmatizer and PorterStemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Ensure `directions` is a list\n",
    "if isinstance(df['directions'][0], str):\n",
    "    import ast\n",
    "    df['directions'] = df['directions'].apply(ast.literal_eval)\n",
    "\n",
    "# Get the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Segment, tokenize, remove stop words, and apply both stemming and lemmatization\n",
    "segmented_tokenized_directions = []\n",
    "for index, row in df.iterrows():\n",
    "    for step in row['directions']:\n",
    "        # Segment the step into smaller parts\n",
    "        segments = step.split('. ')\n",
    "        for segment in segments:\n",
    "            # Clean whitespace and ensure the segment is not empty\n",
    "            clean_segment = segment.strip()\n",
    "            if clean_segment:\n",
    "                try:\n",
    "                    # Tokenize the segment into words\n",
    "                    tokens = word_tokenize(clean_segment)\n",
    "                    \n",
    "                    # Remove stop words from the token list\n",
    "                    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "                    \n",
    "                    # Apply lemmatization first\n",
    "                    lemmatized_tokens = [lemmatizer.lemmatize(word.lower()) for word in filtered_tokens]\n",
    "                    \n",
    "                    # Apply stemming to lemmatized tokens\n",
    "                    stemmed_tokens = [stemmer.stem(word) for word in lemmatized_tokens]\n",
    "                    \n",
    "                    segmented_tokenized_directions.append({\n",
    "                        \"title\": row[\"title\"],\n",
    "                        \"direction_segment\": clean_segment,\n",
    "                        \"tokens_after_lemmatization\": lemmatized_tokens,\n",
    "                        \"tokens_after_stemming\": stemmed_tokens\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing: {clean_segment}. Error: {e}\")\n",
    "\n",
    "# Convert the results into a DataFrame\n",
    "segmented_tokenized_df = pd.DataFrame(segmented_tokenized_directions)\n",
    "\n",
    "# Convert the token columns to strings for proper CSV export\n",
    "segmented_tokenized_df['tokens_after_lemmatization'] = segmented_tokenized_df['tokens_after_lemmatization'].apply(lambda x: ' '.join(x))\n",
    "segmented_tokenized_df['tokens_after_stemming'] = segmented_tokenized_df['tokens_after_stemming'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Export the segmented, tokenized, and processed DataFrame to a CSV file\n",
    "segmented_tokenized_df.to_csv(\"../files/5_lemmatization.csv\", index=False)\n",
    "\n",
    "# Print the first few rows for verification\n",
    "print(segmented_tokenized_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-speech (POS) tagging is the process of labeling words in a text with their corresponding parts of speech in natural language processing (NLP). It helps algorithms understand the grammatical structure and meaning of a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk import pos_tag\n",
    "import ast\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize the WordNetLemmatizer and PorterStemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Verify the directions column\n",
    "print(\"Sample directions column:\")\n",
    "print(df['directions'].iloc[0])\n",
    "print(type(df['directions'].iloc[0]))\n",
    "\n",
    "# Convert directions to lists if necessary\n",
    "if isinstance(df['directions'][0], str):\n",
    "    try:\n",
    "        df['directions'] = df['directions'].apply(ast.literal_eval)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting directions: {e}\")\n",
    "\n",
    "# Get the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Process directions\n",
    "segmented_tokenized_directions = []\n",
    "for index, row in df.iterrows():\n",
    "    for step in row['directions']:\n",
    "        segments = step.split('. ')\n",
    "        for segment in segments:\n",
    "            clean_segment = segment.strip()\n",
    "            print(f\"Processing segment: '{clean_segment}'\")\n",
    "            if clean_segment:\n",
    "                try:\n",
    "                    tokens = word_tokenize(clean_segment)\n",
    "                    print(f\"Tokens: {tokens}\")\n",
    "\n",
    "                    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "                    lemmatized_tokens = [lemmatizer.lemmatize(word.lower()) for word in filtered_tokens]\n",
    "                    stemmed_tokens = [stemmer.stem(word) for word in lemmatized_tokens]\n",
    "                    pos_tags = pos_tag(lemmatized_tokens)\n",
    "\n",
    "                    segmented_tokenized_directions.append({\n",
    "                        \"title\": row[\"title\"],\n",
    "                        \"direction_segment\": clean_segment,\n",
    "                        \"tokens_after_lemmatization\": lemmatized_tokens,\n",
    "                        \"tokens_after_stemming\": stemmed_tokens,\n",
    "                        \"POS_tags\": pos_tags\n",
    "                    })\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing row {index} segment: '{clean_segment}'. Error: {e}\")\n",
    "\n",
    "# Check if any data was generated\n",
    "if not segmented_tokenized_directions:\n",
    "    raise ValueError(\"No valid data was generated. Please check your input data or processing logic.\")\n",
    "\n",
    "# Convert to DataFrame and export\n",
    "segmented_tokenized_df = pd.DataFrame(segmented_tokenized_directions)\n",
    "segmented_tokenized_df.to_csv(\"../files/6_pos_tagging.csv\", index=False)\n",
    "print(segmented_tokenized_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In natural language processing (NLP), feature extraction is a fundamental task that involves converting raw text data into a format that can be easily processed by machine learning algorithms. There are various techniques available for feature extraction in NLP, each with its own strengths and weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Kombino të gjitha segmentet e \"directions\" për çdo recetë\n",
    "df['all_directions'] = df['directions'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Përdor TF-IDF për ekstraktim të veçorive\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=10)  # Top 10 fjalë\n",
    "tfidf_matrix = vectorizer.fit_transform(df['all_directions'])\n",
    "\n",
    "# Konverto TF-IDF në DataFrame për lexim të lehtë\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "tfidf_df['title'] = df['title']\n",
    "\n",
    "# Printo rezultatet\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named entity recognition (NER) is a natural language processing (NLP) method that extracts information from text. NER involves detecting and categorizing important information in text known as named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize NLP tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Custom NER component\n",
    "@Language.component(\"custom_ner\")\n",
    "def custom_ner(doc):\n",
    "    ingredient_list = nlp.get_pipe(\"custom_ner\").cfg[\"ingredient_list\"]\n",
    "    spans = []\n",
    "    for token in doc:\n",
    "        if token.text.lower() in ingredient_list:\n",
    "            spans.append(Span(doc, token.i, token.i + 1, label=\"INGREDIENT\"))\n",
    "    doc.ents = list(doc.ents) + spans  # Add custom entities to SpaCy's entities\n",
    "    return doc\n",
    "\n",
    "# Convert `ingredients` and `directions` columns to lists\n",
    "for col in ['ingredients', 'directions']:\n",
    "    if isinstance(df[col][0], str):\n",
    "        df[col] = df[col].apply(ast.literal_eval)\n",
    "\n",
    "# Dynamically generate the ingredient list\n",
    "ingredient_list = set()\n",
    "for ingredients in df['ingredients']:\n",
    "    for ingredient in ingredients:\n",
    "        tokens = word_tokenize(ingredient.lower())  # Tokenize each ingredient\n",
    "        filtered_tokens = [word for word in tokens if word.isalpha()]  # Keep only alphabetic words\n",
    "        ingredient_list.update(filtered_tokens)  # Add to the ingredient list\n",
    "\n",
    "ingredient_list = list(ingredient_list)  # Convert to a list\n",
    "\n",
    "print(\"Generated Ingredient List:\", ingredient_list)\n",
    "\n",
    "# Add custom NER to SpaCy pipeline\n",
    "nlp.add_pipe(\"custom_ner\", last=True)\n",
    "nlp.get_pipe(\"custom_ner\").cfg = {\"ingredient_list\": ingredient_list}  # Add ingredient list to the pipe config\n",
    "\n",
    "# NLP Processing and NER Extraction\n",
    "ner_results = []\n",
    "for index, row in df.iterrows():\n",
    "    combined_text = ' '.join(row['directions'])  # Combine all steps into one string\n",
    "    doc = nlp(combined_text)  # Process text using SpaCy\n",
    "    \n",
    "    # Extract entities\n",
    "    ner_list = []\n",
    "    for ent in doc.ents:\n",
    "        ner_list.append(f\"{ent.text} ({ent.label_})\")\n",
    "    \n",
    "    ner_results.append(', '.join(ner_list))\n",
    "\n",
    "# Add the NER results to the dataset\n",
    "df['NLP_NER'] = ner_results\n",
    "\n",
    "# Export to CSV\n",
    "df[['title', 'ingredients', 'NER', 'NLP_NER']].to_csv(\"../files/final_nlp_ner_results.csv\", index=False)\n",
    "\n",
    "# Print the results\n",
    "print(df[['title', 'NLP_NER']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
