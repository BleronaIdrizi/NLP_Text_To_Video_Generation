{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General-purpose libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Text-to-Video and NLP Libraries\n",
    "from transformers import pipeline\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# Video and Image Handling\n",
    "from moviepy import ImageSequenceClip, AudioFileClip\n",
    "import cv2  # OpenCV for image manipulation\n",
    "\n",
    "# Text-to-Speech\n",
    "from gtts import gTTS\n",
    "\n",
    "# Miscellaneous\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentation in NLP involves breaking down a larger piece of text into smaller, meaningful units such as sentences or paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the type of the `directions` column\n",
    "print(type(df['directions'][0]))\n",
    "\n",
    "# If `directions` is already a list, skip the parsing step\n",
    "# Flatten and segment the directions into smaller pieces\n",
    "segmented_directions = []\n",
    "for index, row in df.iterrows():\n",
    "    for step in row['directions']:\n",
    "        # Split steps into smaller segments using `.split('. ')`\n",
    "        segments = step.split('. ')\n",
    "        for segment in segments:\n",
    "            # Avoid adding empty strings\n",
    "            if segment.strip():\n",
    "                segmented_directions.append({\n",
    "                    \"title\": row[\"title\"],\n",
    "                    \"direction_segment\": segment.strip()  # Clean whitespace\n",
    "                })\n",
    "\n",
    "# Convert the segmented directions into a new DataFrame\n",
    "segmented_df = pd.DataFrame(segmented_directions)\n",
    "\n",
    "# Export the segmented DataFrame to a CSV file\n",
    "segmented_df.to_csv(\"../files/1_segmentation.csv\", index=False)\n",
    "\n",
    "# Print the first few rows for verification\n",
    "print(segmented_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenization is used in MLP to split paragraphs and sentences into smaller units that can be more easily assigned meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Check if `directions` is a list or a string\n",
    "if isinstance(df['directions'][0], str):\n",
    "    import ast\n",
    "    df['directions'] = df['directions'].apply(ast.literal_eval)\n",
    "\n",
    "# Segment and tokenize the directions\n",
    "segmented_tokenized_directions = []\n",
    "for index, row in df.iterrows():\n",
    "    for step in row['directions']:\n",
    "        # Segment the step into smaller parts\n",
    "        segments = step.split('. ')\n",
    "        for segment in segments:\n",
    "            # Clean whitespace and ensure the segment is not empty\n",
    "            clean_segment = segment.strip()\n",
    "            if clean_segment:\n",
    "                try:\n",
    "                    # Tokenize the segment into words\n",
    "                    tokens = word_tokenize(clean_segment)\n",
    "                    segmented_tokenized_directions.append({\n",
    "                        \"title\": row[\"title\"],\n",
    "                        \"direction_segment\": clean_segment,\n",
    "                        \"tokens\": tokens\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error tokenizing: {clean_segment}. Error: {e}\")\n",
    "\n",
    "# Convert the results into a DataFrame\n",
    "segmented_tokenized_df = pd.DataFrame(segmented_tokenized_directions)\n",
    "\n",
    "# Convert the tokens column to a string for proper CSV export\n",
    "segmented_tokenized_df['tokens'] = segmented_tokenized_df['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Export the segmented and tokenized DataFrame to a CSV file\n",
    "segmented_tokenized_df.to_csv(\"../files/2_tokenization.csv\", index=False)\n",
    "\n",
    "# Print the first few rows for verification\n",
    "print(segmented_tokenized_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are a set of commonly used words in a language. Examples of stop words in English are “a,” “the,” “is,” “are,” etc. Stop words are commonly used in Text Mining and Natural Language Processing (NLP) to eliminate words that are so widely used that they carry very little useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')       # Tokenizer\n",
    "nltk.download('stopwords')   # Stop words list\n",
    "\n",
    "# Ensure `directions` is a list\n",
    "if isinstance(df['directions'][0], str):\n",
    "    import ast\n",
    "    df['directions'] = df['directions'].apply(ast.literal_eval)\n",
    "\n",
    "# Get the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Segment, tokenize, and remove stop words from directions\n",
    "segmented_tokenized_directions = []\n",
    "for index, row in df.iterrows():\n",
    "    for step in row['directions']:\n",
    "        # Segment the step into smaller parts\n",
    "        segments = step.split('. ')\n",
    "        for segment in segments:\n",
    "            # Clean whitespace and ensure the segment is not empty\n",
    "            clean_segment = segment.strip()\n",
    "            if clean_segment:\n",
    "                try:\n",
    "                    # Tokenize the segment into words\n",
    "                    tokens = word_tokenize(clean_segment)\n",
    "                    \n",
    "                    # Remove stop words from the token list\n",
    "                    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "                    \n",
    "                    segmented_tokenized_directions.append({\n",
    "                        \"title\": row[\"title\"],\n",
    "                        \"direction_segment\": clean_segment,\n",
    "                        \"tokens\": filtered_tokens\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error tokenizing: {clean_segment}. Error: {e}\")\n",
    "\n",
    "# Convert the results into a DataFrame\n",
    "segmented_tokenized_df = pd.DataFrame(segmented_tokenized_directions)\n",
    "\n",
    "# Convert the tokens column to a string for proper CSV export\n",
    "segmented_tokenized_df['tokens'] = segmented_tokenized_df['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Export the segmented and tokenized DataFrame to a CSV file\n",
    "segmented_tokenized_df.to_csv(\"../files/3_stopwords.csv\", index=False)\n",
    "\n",
    "# Print the first few rows for verification\n",
    "print(segmented_tokenized_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is a text preprocessing technique in natural language processing (NLP). Specifically, it is the process of reducing inflected form of a word to one so-called “stem,” or root form, also known as a “lemma” in linguistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')       # Tokenizer\n",
    "nltk.download('stopwords')   # Stop words list\n",
    "\n",
    "# Initialize the PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Ensure `directions` is a list\n",
    "if isinstance(df['directions'][0], str):\n",
    "    import ast\n",
    "    df['directions'] = df['directions'].apply(ast.literal_eval)\n",
    "\n",
    "# Get the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Segment, tokenize, remove stop words, and apply stemming\n",
    "segmented_tokenized_directions = []\n",
    "for index, row in df.iterrows():\n",
    "    for step in row['directions']:\n",
    "        # Segment the step into smaller parts\n",
    "        segments = step.split('. ')\n",
    "        for segment in segments:\n",
    "            # Clean whitespace and ensure the segment is not empty\n",
    "            clean_segment = segment.strip()\n",
    "            if clean_segment:\n",
    "                try:\n",
    "                    # Tokenize the segment into words\n",
    "                    tokens = word_tokenize(clean_segment)\n",
    "                    \n",
    "                    # Remove stop words from the token list\n",
    "                    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "                    \n",
    "                    # Apply stemming to the filtered tokens\n",
    "                    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "                    \n",
    "                    segmented_tokenized_directions.append({\n",
    "                        \"title\": row[\"title\"],\n",
    "                        \"direction_segment\": clean_segment,\n",
    "                        \"tokens\": stemmed_tokens\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error tokenizing: {clean_segment}. Error: {e}\")\n",
    "\n",
    "# Convert the results into a DataFrame\n",
    "segmented_tokenized_df = pd.DataFrame(segmented_tokenized_directions)\n",
    "\n",
    "# Convert the tokens column to a string for proper CSV export\n",
    "segmented_tokenized_df['tokens'] = segmented_tokenized_df['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Export the segmented and tokenized DataFrame to a CSV file\n",
    "segmented_tokenized_df.to_csv(\"../files/4_stemming.csv\", index=False)\n",
    "\n",
    "# Print the first few rows for verification\n",
    "print(segmented_tokenized_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is a text pre-processing technique used in natural language processing (NLP) models to break a word down to its root meaning to identify similarities. For example, a lemmatization algorithm would reduce the word better to its root word, or lemme, good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')       # Tokenizer\n",
    "nltk.download('stopwords')   # Stop words list\n",
    "nltk.download('wordnet')     # WordNet for lemmatization\n",
    "nltk.download('omw-1.4')     # WordNet lemmatizer's dependency\n",
    "\n",
    "# Initialize the WordNetLemmatizer and PorterStemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Ensure `directions` is a list\n",
    "if isinstance(df['directions'][0], str):\n",
    "    import ast\n",
    "    df['directions'] = df['directions'].apply(ast.literal_eval)\n",
    "\n",
    "# Get the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Segment, tokenize, remove stop words, and apply both stemming and lemmatization\n",
    "segmented_tokenized_directions = []\n",
    "for index, row in df.iterrows():\n",
    "    for step in row['directions']:\n",
    "        # Segment the step into smaller parts\n",
    "        segments = step.split('. ')\n",
    "        for segment in segments:\n",
    "            # Clean whitespace and ensure the segment is not empty\n",
    "            clean_segment = segment.strip()\n",
    "            if clean_segment:\n",
    "                try:\n",
    "                    # Tokenize the segment into words\n",
    "                    tokens = word_tokenize(clean_segment)\n",
    "                    \n",
    "                    # Remove stop words from the token list\n",
    "                    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "                    \n",
    "                    # Apply lemmatization first\n",
    "                    lemmatized_tokens = [lemmatizer.lemmatize(word.lower()) for word in filtered_tokens]\n",
    "                    \n",
    "                    # Apply stemming to lemmatized tokens\n",
    "                    stemmed_tokens = [stemmer.stem(word) for word in lemmatized_tokens]\n",
    "                    \n",
    "                    segmented_tokenized_directions.append({\n",
    "                        \"title\": row[\"title\"],\n",
    "                        \"direction_segment\": clean_segment,\n",
    "                        \"tokens_after_lemmatization\": lemmatized_tokens,\n",
    "                        \"tokens_after_stemming\": stemmed_tokens\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing: {clean_segment}. Error: {e}\")\n",
    "\n",
    "# Convert the results into a DataFrame\n",
    "segmented_tokenized_df = pd.DataFrame(segmented_tokenized_directions)\n",
    "\n",
    "# Convert the token columns to strings for proper CSV export\n",
    "segmented_tokenized_df['tokens_after_lemmatization'] = segmented_tokenized_df['tokens_after_lemmatization'].apply(lambda x: ' '.join(x))\n",
    "segmented_tokenized_df['tokens_after_stemming'] = segmented_tokenized_df['tokens_after_stemming'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Export the segmented, tokenized, and processed DataFrame to a CSV file\n",
    "segmented_tokenized_df.to_csv(\"../files/5_lemmatization.csv\", index=False)\n",
    "\n",
    "# Print the first few rows for verification\n",
    "print(segmented_tokenized_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-speech (POS) tagging is the process of labeling words in a text with their corresponding parts of speech in natural language processing (NLP). It helps algorithms understand the grammatical structure and meaning of a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk import pos_tag\n",
    "import ast\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize the WordNetLemmatizer and PorterStemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Verify the directions column\n",
    "print(\"Sample directions column:\")\n",
    "print(df['directions'].iloc[0])\n",
    "print(type(df['directions'].iloc[0]))\n",
    "\n",
    "# Convert directions to lists if necessary\n",
    "if isinstance(df['directions'][0], str):\n",
    "    try:\n",
    "        df['directions'] = df['directions'].apply(ast.literal_eval)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting directions: {e}\")\n",
    "\n",
    "# Get the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Process directions\n",
    "segmented_tokenized_directions = []\n",
    "for index, row in df.iterrows():\n",
    "    for step in row['directions']:\n",
    "        segments = step.split('. ')\n",
    "        for segment in segments:\n",
    "            clean_segment = segment.strip()\n",
    "            print(f\"Processing segment: '{clean_segment}'\")\n",
    "            if clean_segment:\n",
    "                try:\n",
    "                    tokens = word_tokenize(clean_segment)\n",
    "                    print(f\"Tokens: {tokens}\")\n",
    "\n",
    "                    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "                    lemmatized_tokens = [lemmatizer.lemmatize(word.lower()) for word in filtered_tokens]\n",
    "                    stemmed_tokens = [stemmer.stem(word) for word in lemmatized_tokens]\n",
    "                    pos_tags = pos_tag(lemmatized_tokens)\n",
    "\n",
    "                    segmented_tokenized_directions.append({\n",
    "                        \"title\": row[\"title\"],\n",
    "                        \"direction_segment\": clean_segment,\n",
    "                        \"tokens_after_lemmatization\": lemmatized_tokens,\n",
    "                        \"tokens_after_stemming\": stemmed_tokens,\n",
    "                        \"POS_tags\": pos_tags\n",
    "                    })\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing row {index} segment: '{clean_segment}'. Error: {e}\")\n",
    "\n",
    "# Check if any data was generated\n",
    "if not segmented_tokenized_directions:\n",
    "    raise ValueError(\"No valid data was generated. Please check your input data or processing logic.\")\n",
    "\n",
    "# Convert to DataFrame and export\n",
    "segmented_tokenized_df = pd.DataFrame(segmented_tokenized_directions)\n",
    "segmented_tokenized_df.to_csv(\"../files/6_pos_tagging.csv\", index=False)\n",
    "print(segmented_tokenized_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In natural language processing (NLP), feature extraction is a fundamental task that involves converting raw text data into a format that can be easily processed by machine learning algorithms. There are various techniques available for feature extraction in NLP, each with its own strengths and weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Kombino të gjitha segmentet e \"directions\" për çdo recetë\n",
    "df['all_directions'] = df['directions'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Përdor TF-IDF për ekstraktim të veçorive\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=10)  # Top 10 fjalë\n",
    "tfidf_matrix = vectorizer.fit_transform(df['all_directions'])\n",
    "\n",
    "# Konverto TF-IDF në DataFrame për lexim të lehtë\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "tfidf_df['title'] = df['title']\n",
    "\n",
    "# Printo rezultatet\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named entity recognition (NER) is a natural language processing (NLP) method that extracts information from text. NER involves detecting and categorizing important information in text known as named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize NLP tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Custom NER component\n",
    "@Language.component(\"custom_ner\")\n",
    "def custom_ner(doc):\n",
    "    ingredient_list = nlp.get_pipe(\"custom_ner\").cfg[\"ingredient_list\"]\n",
    "    spans = []\n",
    "    for token in doc:\n",
    "        if token.text.lower() in ingredient_list:\n",
    "            spans.append(Span(doc, token.i, token.i + 1, label=\"INGREDIENT\"))\n",
    "    doc.ents = list(doc.ents) + spans  # Add custom entities to SpaCy's entities\n",
    "    return doc\n",
    "\n",
    "# Convert `ingredients` and `directions` columns to lists\n",
    "for col in ['ingredients', 'directions']:\n",
    "    if isinstance(df[col][0], str):\n",
    "        df[col] = df[col].apply(ast.literal_eval)\n",
    "\n",
    "# Dynamically generate the ingredient list\n",
    "ingredient_list = set()\n",
    "for ingredients in df['ingredients']:\n",
    "    for ingredient in ingredients:\n",
    "        tokens = word_tokenize(ingredient.lower())  # Tokenize each ingredient\n",
    "        filtered_tokens = [word for word in tokens if word.isalpha()]  # Keep only alphabetic words\n",
    "        ingredient_list.update(filtered_tokens)  # Add to the ingredient list\n",
    "\n",
    "ingredient_list = list(ingredient_list)  # Convert to a list\n",
    "\n",
    "print(\"Generated Ingredient List:\", ingredient_list)\n",
    "\n",
    "# Add custom NER to SpaCy pipeline\n",
    "nlp.add_pipe(\"custom_ner\", last=True)\n",
    "nlp.get_pipe(\"custom_ner\").cfg = {\"ingredient_list\": ingredient_list}  # Add ingredient list to the pipe config\n",
    "\n",
    "# NLP Processing and NER Extraction\n",
    "ner_results = []\n",
    "for index, row in df.iterrows():\n",
    "    combined_text = ' '.join(row['directions'])  # Combine all steps into one string\n",
    "    doc = nlp(combined_text)  # Process text using SpaCy\n",
    "    \n",
    "    # Extract entities\n",
    "    ner_list = []\n",
    "    for ent in doc.ents:\n",
    "        ner_list.append(f\"{ent.text} ({ent.label_})\")\n",
    "    \n",
    "    ner_results.append(', '.join(ner_list))\n",
    "\n",
    "# Add the NER results to the dataset\n",
    "df['NLP_NER'] = ner_results\n",
    "\n",
    "# Export to CSV\n",
    "df[['title', 'ingredients', 'NER', 'NLP_NER']].to_csv(\"../files/final_nlp_ner_results.csv\", index=False)\n",
    "\n",
    "# Print the results\n",
    "print(df[['title', 'NLP_NER']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
