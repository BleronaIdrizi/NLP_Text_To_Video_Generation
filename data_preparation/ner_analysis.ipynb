{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bleronaidrizi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/bleronaidrizi/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bleronaidrizi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/bleronaidrizi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Span\n",
    "import nltk\n",
    "nltk.download('punkt')  # Only needed the first time\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../files/recipes_data1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentation in NLP involves breaking down a larger piece of text into smaller, meaningful units such as sentences or paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "                 title direction_segment\n",
      "0  No-Bake Nut Cookies                 [\n",
      "1  No-Bake Nut Cookies                 \"\n",
      "2  No-Bake Nut Cookies                 I\n",
      "3  No-Bake Nut Cookies                 n\n",
      "4  No-Bake Nut Cookies                 a\n"
     ]
    }
   ],
   "source": [
    "# Check the type of the `directions` column\n",
    "print(type(df['directions'][0]))\n",
    "\n",
    "# If `directions` is already a list, skip the parsing step\n",
    "# Flatten and segment the directions into smaller pieces\n",
    "segmented_directions = []\n",
    "for index, row in df.iterrows():\n",
    "    for step in row['directions']:\n",
    "        # Split steps into smaller segments using `.split('. ')`\n",
    "        segments = step.split('. ')\n",
    "        for segment in segments:\n",
    "            # Avoid adding empty strings\n",
    "            if segment.strip():\n",
    "                segmented_directions.append({\n",
    "                    \"title\": row[\"title\"],\n",
    "                    \"direction_segment\": segment.strip()  # Clean whitespace\n",
    "                })\n",
    "\n",
    "# Convert the segmented directions into a new DataFrame\n",
    "segmented_df = pd.DataFrame(segmented_directions)\n",
    "\n",
    "# Export the segmented DataFrame to a CSV file\n",
    "segmented_df.to_csv(\"../files/1_segmentation.csv\", index=False)\n",
    "\n",
    "# Print the first few rows for verification\n",
    "print(segmented_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is used in MLP to split paragraphs and sentences into smaller units that can be more easily assigned meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 title                                  direction_segment  \\\n",
      "0  No-Bake Nut Cookies  In a heavy 2-quart saucepan, mix brown sugar, ...   \n",
      "1  No-Bake Nut Cookies  Stir over medium heat until mixture bubbles al...   \n",
      "2  No-Bake Nut Cookies                       Boil and stir 5 minutes more   \n",
      "3  No-Bake Nut Cookies                                     Take off heat.   \n",
      "4  No-Bake Nut Cookies              Stir in vanilla and cereal; mix well.   \n",
      "\n",
      "                                              tokens  \n",
      "0  In a heavy 2-quart saucepan , mix brown sugar ...  \n",
      "1  Stir over medium heat until mixture bubbles al...  \n",
      "2                       Boil and stir 5 minutes more  \n",
      "3                                    Take off heat .  \n",
      "4            Stir in vanilla and cereal ; mix well .  \n"
     ]
    }
   ],
   "source": [
    "# Check if `directions` is a list or a string\n",
    "if isinstance(df['directions'][0], str):\n",
    "    import ast\n",
    "    df['directions'] = df['directions'].apply(ast.literal_eval)\n",
    "\n",
    "# Segment and tokenize the directions\n",
    "segmented_tokenized_directions = []\n",
    "for index, row in df.iterrows():\n",
    "    for step in row['directions']:\n",
    "        # Segment the step into smaller parts\n",
    "        segments = step.split('. ')\n",
    "        for segment in segments:\n",
    "            # Clean whitespace and ensure the segment is not empty\n",
    "            clean_segment = segment.strip()\n",
    "            if clean_segment:\n",
    "                try:\n",
    "                    # Tokenize the segment into words\n",
    "                    tokens = word_tokenize(clean_segment)\n",
    "                    segmented_tokenized_directions.append({\n",
    "                        \"title\": row[\"title\"],\n",
    "                        \"direction_segment\": clean_segment,\n",
    "                        \"tokens\": tokens\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error tokenizing: {clean_segment}. Error: {e}\")\n",
    "\n",
    "# Convert the results into a DataFrame\n",
    "segmented_tokenized_df = pd.DataFrame(segmented_tokenized_directions)\n",
    "\n",
    "# Convert the tokens column to a string for proper CSV export\n",
    "segmented_tokenized_df['tokens'] = segmented_tokenized_df['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Export the segmented and tokenized DataFrame to a CSV file\n",
    "segmented_tokenized_df.to_csv(\"../files/2_tokenization.csv\", index=False)\n",
    "\n",
    "# Print the first few rows for verification\n",
    "print(segmented_tokenized_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are a set of commonly used words in a language. Examples of stop words in English are “a,” “the,” “is,” “are,” etc. Stop words are commonly used in Text Mining and Natural Language Processing (NLP) to eliminate words that are so widely used that they carry very little useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 title                                  direction_segment  \\\n",
      "0  No-Bake Nut Cookies  In a heavy 2-quart saucepan, mix brown sugar, ...   \n",
      "1  No-Bake Nut Cookies  Stir over medium heat until mixture bubbles al...   \n",
      "2  No-Bake Nut Cookies                       Boil and stir 5 minutes more   \n",
      "3  No-Bake Nut Cookies                                     Take off heat.   \n",
      "4  No-Bake Nut Cookies              Stir in vanilla and cereal; mix well.   \n",
      "\n",
      "                                              tokens  \n",
      "0  heavy 2-quart saucepan , mix brown sugar , nut...  \n",
      "1             Stir medium heat mixture bubbles top .  \n",
      "2                                Boil stir 5 minutes  \n",
      "3                                        Take heat .  \n",
      "4                   Stir vanilla cereal ; mix well .  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bleronaidrizi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bleronaidrizi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')       # Tokenizer\n",
    "nltk.download('stopwords')   # Stop words list\n",
    "\n",
    "# Ensure `directions` is a list\n",
    "if isinstance(df['directions'][0], str):\n",
    "    import ast\n",
    "    df['directions'] = df['directions'].apply(ast.literal_eval)\n",
    "\n",
    "# Get the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Segment, tokenize, and remove stop words from directions\n",
    "segmented_tokenized_directions = []\n",
    "for index, row in df.iterrows():\n",
    "    for step in row['directions']:\n",
    "        # Segment the step into smaller parts\n",
    "        segments = step.split('. ')\n",
    "        for segment in segments:\n",
    "            # Clean whitespace and ensure the segment is not empty\n",
    "            clean_segment = segment.strip()\n",
    "            if clean_segment:\n",
    "                try:\n",
    "                    # Tokenize the segment into words\n",
    "                    tokens = word_tokenize(clean_segment)\n",
    "                    \n",
    "                    # Remove stop words from the token list\n",
    "                    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "                    \n",
    "                    segmented_tokenized_directions.append({\n",
    "                        \"title\": row[\"title\"],\n",
    "                        \"direction_segment\": clean_segment,\n",
    "                        \"tokens\": filtered_tokens\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error tokenizing: {clean_segment}. Error: {e}\")\n",
    "\n",
    "# Convert the results into a DataFrame\n",
    "segmented_tokenized_df = pd.DataFrame(segmented_tokenized_directions)\n",
    "\n",
    "# Convert the tokens column to a string for proper CSV export\n",
    "segmented_tokenized_df['tokens'] = segmented_tokenized_df['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Export the segmented and tokenized DataFrame to a CSV file\n",
    "segmented_tokenized_df.to_csv(\"../files/3_stopwords.csv\", index=False)\n",
    "\n",
    "# Print the first few rows for verification\n",
    "print(segmented_tokenized_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is a text preprocessing technique in natural language processing (NLP). Specifically, it is the process of reducing inflected form of a word to one so-called “stem,” or root form, also known as a “lemma” in linguistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 title                                  direction_segment  \\\n",
      "0  No-Bake Nut Cookies  In a heavy 2-quart saucepan, mix brown sugar, ...   \n",
      "1  No-Bake Nut Cookies  Stir over medium heat until mixture bubbles al...   \n",
      "2  No-Bake Nut Cookies                       Boil and stir 5 minutes more   \n",
      "3  No-Bake Nut Cookies                                     Take off heat.   \n",
      "4  No-Bake Nut Cookies              Stir in vanilla and cereal; mix well.   \n",
      "\n",
      "                                              tokens  \n",
      "0  heavi 2-quart saucepan , mix brown sugar , nut...  \n",
      "1                stir medium heat mixtur bubbl top .  \n",
      "2                                  boil stir 5 minut  \n",
      "3                                        take heat .  \n",
      "4                   stir vanilla cereal ; mix well .  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bleronaidrizi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bleronaidrizi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')       # Tokenizer\n",
    "nltk.download('stopwords')   # Stop words list\n",
    "\n",
    "# Initialize the PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Ensure `directions` is a list\n",
    "if isinstance(df['directions'][0], str):\n",
    "    import ast\n",
    "    df['directions'] = df['directions'].apply(ast.literal_eval)\n",
    "\n",
    "# Get the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Segment, tokenize, remove stop words, and apply stemming\n",
    "segmented_tokenized_directions = []\n",
    "for index, row in df.iterrows():\n",
    "    for step in row['directions']:\n",
    "        # Segment the step into smaller parts\n",
    "        segments = step.split('. ')\n",
    "        for segment in segments:\n",
    "            # Clean whitespace and ensure the segment is not empty\n",
    "            clean_segment = segment.strip()\n",
    "            if clean_segment:\n",
    "                try:\n",
    "                    # Tokenize the segment into words\n",
    "                    tokens = word_tokenize(clean_segment)\n",
    "                    \n",
    "                    # Remove stop words from the token list\n",
    "                    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "                    \n",
    "                    # Apply stemming to the filtered tokens\n",
    "                    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "                    \n",
    "                    segmented_tokenized_directions.append({\n",
    "                        \"title\": row[\"title\"],\n",
    "                        \"direction_segment\": clean_segment,\n",
    "                        \"tokens\": stemmed_tokens\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error tokenizing: {clean_segment}. Error: {e}\")\n",
    "\n",
    "# Convert the results into a DataFrame\n",
    "segmented_tokenized_df = pd.DataFrame(segmented_tokenized_directions)\n",
    "\n",
    "# Convert the tokens column to a string for proper CSV export\n",
    "segmented_tokenized_df['tokens'] = segmented_tokenized_df['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Export the segmented and tokenized DataFrame to a CSV file\n",
    "segmented_tokenized_df.to_csv(\"../files/4_stemming.csv\", index=False)\n",
    "\n",
    "# Print the first few rows for verification\n",
    "print(segmented_tokenized_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is a text pre-processing technique used in natural language processing (NLP) models to break a word down to its root meaning to identify similarities. For example, a lemmatization algorithm would reduce the word better to its root word, or lemme, good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bleronaidrizi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bleronaidrizi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/bleronaidrizi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/bleronaidrizi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 title                                  direction_segment  \\\n",
      "0  No-Bake Nut Cookies  In a heavy 2-quart saucepan, mix brown sugar, ...   \n",
      "1  No-Bake Nut Cookies  Stir over medium heat until mixture bubbles al...   \n",
      "2  No-Bake Nut Cookies                       Boil and stir 5 minutes more   \n",
      "3  No-Bake Nut Cookies                                     Take off heat.   \n",
      "4  No-Bake Nut Cookies              Stir in vanilla and cereal; mix well.   \n",
      "\n",
      "                          tokens_after_lemmatization  \\\n",
      "0  heavy 2-quart saucepan , mix brown sugar , nut...   \n",
      "1              stir medium heat mixture bubble top .   \n",
      "2                                 boil stir 5 minute   \n",
      "3                                        take heat .   \n",
      "4                   stir vanilla cereal ; mix well .   \n",
      "\n",
      "                               tokens_after_stemming  \n",
      "0  heavi 2-quart saucepan , mix brown sugar , nut...  \n",
      "1                stir medium heat mixtur bubbl top .  \n",
      "2                                  boil stir 5 minut  \n",
      "3                                        take heat .  \n",
      "4                   stir vanilla cereal ; mix well .  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')       # Tokenizer\n",
    "nltk.download('stopwords')   # Stop words list\n",
    "nltk.download('wordnet')     # WordNet for lemmatization\n",
    "nltk.download('omw-1.4')     # WordNet lemmatizer's dependency\n",
    "\n",
    "# Initialize the WordNetLemmatizer and PorterStemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Ensure `directions` is a list\n",
    "if isinstance(df['directions'][0], str):\n",
    "    import ast\n",
    "    df['directions'] = df['directions'].apply(ast.literal_eval)\n",
    "\n",
    "# Get the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Segment, tokenize, remove stop words, and apply both stemming and lemmatization\n",
    "segmented_tokenized_directions = []\n",
    "for index, row in df.iterrows():\n",
    "    for step in row['directions']:\n",
    "        # Segment the step into smaller parts\n",
    "        segments = step.split('. ')\n",
    "        for segment in segments:\n",
    "            # Clean whitespace and ensure the segment is not empty\n",
    "            clean_segment = segment.strip()\n",
    "            if clean_segment:\n",
    "                try:\n",
    "                    # Tokenize the segment into words\n",
    "                    tokens = word_tokenize(clean_segment)\n",
    "                    \n",
    "                    # Remove stop words from the token list\n",
    "                    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "                    \n",
    "                    # Apply lemmatization first\n",
    "                    lemmatized_tokens = [lemmatizer.lemmatize(word.lower()) for word in filtered_tokens]\n",
    "                    \n",
    "                    # Apply stemming to lemmatized tokens\n",
    "                    stemmed_tokens = [stemmer.stem(word) for word in lemmatized_tokens]\n",
    "                    \n",
    "                    segmented_tokenized_directions.append({\n",
    "                        \"title\": row[\"title\"],\n",
    "                        \"direction_segment\": clean_segment,\n",
    "                        \"tokens_after_lemmatization\": lemmatized_tokens,\n",
    "                        \"tokens_after_stemming\": stemmed_tokens\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing: {clean_segment}. Error: {e}\")\n",
    "\n",
    "# Convert the results into a DataFrame\n",
    "segmented_tokenized_df = pd.DataFrame(segmented_tokenized_directions)\n",
    "\n",
    "# Convert the token columns to strings for proper CSV export\n",
    "segmented_tokenized_df['tokens_after_lemmatization'] = segmented_tokenized_df['tokens_after_lemmatization'].apply(lambda x: ' '.join(x))\n",
    "segmented_tokenized_df['tokens_after_stemming'] = segmented_tokenized_df['tokens_after_stemming'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Export the segmented, tokenized, and processed DataFrame to a CSV file\n",
    "segmented_tokenized_df.to_csv(\"../files/5_lemmatization.csv\", index=False)\n",
    "\n",
    "# Print the first few rows for verification\n",
    "print(segmented_tokenized_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-speech (POS) tagging is the process of labeling words in a text with their corresponding parts of speech in natural language processing (NLP). It helps algorithms understand the grammatical structure and meaning of a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample directions column:\n",
      "['In a heavy 2-quart saucepan, mix brown sugar, nuts, evaporated milk and butter or margarine.', 'Stir over medium heat until mixture bubbles all over top.', 'Boil and stir 5 minutes more. Take off heat.', 'Stir in vanilla and cereal; mix well.', 'Using 2 teaspoons, drop and shape into 30 clusters on wax paper.', 'Let stand until firm, about 30 minutes.']\n",
      "<class 'list'>\n",
      "Processing segment: 'In a heavy 2-quart saucepan, mix brown sugar, nuts, evaporated milk and butter or margarine.'\n",
      "Tokens: ['In', 'a', 'heavy', '2-quart', 'saucepan', ',', 'mix', 'brown', 'sugar', ',', 'nuts', ',', 'evaporated', 'milk', 'and', 'butter', 'or', 'margarine', '.']\n",
      "Processing segment: 'Stir over medium heat until mixture bubbles all over top.'\n",
      "Tokens: ['Stir', 'over', 'medium', 'heat', 'until', 'mixture', 'bubbles', 'all', 'over', 'top', '.']\n",
      "Processing segment: 'Boil and stir 5 minutes more'\n",
      "Tokens: ['Boil', 'and', 'stir', '5', 'minutes', 'more']\n",
      "Processing segment: 'Take off heat.'\n",
      "Tokens: ['Take', 'off', 'heat', '.']\n",
      "Processing segment: 'Stir in vanilla and cereal; mix well.'\n",
      "Tokens: ['Stir', 'in', 'vanilla', 'and', 'cereal', ';', 'mix', 'well', '.']\n",
      "Processing segment: 'Using 2 teaspoons, drop and shape into 30 clusters on wax paper.'\n",
      "Tokens: ['Using', '2', 'teaspoons', ',', 'drop', 'and', 'shape', 'into', '30', 'clusters', 'on', 'wax', 'paper', '.']\n",
      "Processing segment: 'Let stand until firm, about 30 minutes.'\n",
      "Tokens: ['Let', 'stand', 'until', 'firm', ',', 'about', '30', 'minutes', '.']\n",
      "Processing segment: 'Place chipped beef on bottom of baking dish.'\n",
      "Tokens: ['Place', 'chipped', 'beef', 'on', 'bottom', 'of', 'baking', 'dish', '.']\n",
      "Processing segment: 'Place chicken on top of beef.'\n",
      "Tokens: ['Place', 'chicken', 'on', 'top', 'of', 'beef', '.']\n",
      "Processing segment: 'Mix soup and cream together; pour over chicken'\n",
      "Tokens: ['Mix', 'soup', 'and', 'cream', 'together', ';', 'pour', 'over', 'chicken']\n",
      "Processing segment: 'Bake, uncovered, at 275° for 3 hours.'\n",
      "Tokens: ['Bake', ',', 'uncovered', ',', 'at', '275°', 'for', '3', 'hours', '.']\n",
      "Processing segment: 'In a slow cooker, combine all ingredients'\n",
      "Tokens: ['In', 'a', 'slow', 'cooker', ',', 'combine', 'all', 'ingredients']\n",
      "Processing segment: 'Cover and cook on low for 4 hours or until heated through and cheese is melted'\n",
      "Tokens: ['Cover', 'and', 'cook', 'on', 'low', 'for', '4', 'hours', 'or', 'until', 'heated', 'through', 'and', 'cheese', 'is', 'melted']\n",
      "Processing segment: 'Stir well before serving'\n",
      "Tokens: ['Stir', 'well', 'before', 'serving']\n",
      "Processing segment: 'Yields 6 servings.'\n",
      "Tokens: ['Yields', '6', 'servings', '.']\n",
      "Processing segment: 'Boil and debone chicken.'\n",
      "Tokens: ['Boil', 'and', 'debone', 'chicken', '.']\n",
      "Processing segment: 'Put bite size pieces in average size square casserole dish.'\n",
      "Tokens: ['Put', 'bite', 'size', 'pieces', 'in', 'average', 'size', 'square', 'casserole', 'dish', '.']\n",
      "Processing segment: 'Pour gravy and cream of mushroom soup over chicken; level.'\n",
      "Tokens: ['Pour', 'gravy', 'and', 'cream', 'of', 'mushroom', 'soup', 'over', 'chicken', ';', 'level', '.']\n",
      "Processing segment: 'Make stuffing according to instructions on box (do not make too moist).'\n",
      "Tokens: ['Make', 'stuffing', 'according', 'to', 'instructions', 'on', 'box', '(', 'do', 'not', 'make', 'too', 'moist', ')', '.']\n",
      "Processing segment: 'Put stuffing on top of chicken and gravy; level.'\n",
      "Tokens: ['Put', 'stuffing', 'on', 'top', 'of', 'chicken', 'and', 'gravy', ';', 'level', '.']\n",
      "Processing segment: 'Sprinkle shredded cheese on top and bake at 350° for approximately 20 minutes or until golden and bubbly.'\n",
      "Tokens: ['Sprinkle', 'shredded', 'cheese', 'on', 'top', 'and', 'bake', 'at', '350°', 'for', 'approximately', '20', 'minutes', 'or', 'until', 'golden', 'and', 'bubbly', '.']\n",
      "Processing segment: 'Combine first four ingredients and press in 13 x 9-inch ungreased pan.'\n",
      "Tokens: ['Combine', 'first', 'four', 'ingredients', 'and', 'press', 'in', '13', 'x', '9-inch', 'ungreased', 'pan', '.']\n",
      "Processing segment: 'Melt chocolate chips and spread over mixture'\n",
      "Tokens: ['Melt', 'chocolate', 'chips', 'and', 'spread', 'over', 'mixture']\n",
      "Processing segment: 'Refrigerate for about 20 minutes and cut into pieces before chocolate gets hard.'\n",
      "Tokens: ['Refrigerate', 'for', 'about', '20', 'minutes', 'and', 'cut', 'into', 'pieces', 'before', 'chocolate', 'gets', 'hard', '.']\n",
      "Processing segment: 'Keep in refrigerator.'\n",
      "Tokens: ['Keep', 'in', 'refrigerator', '.']\n",
      "                 title                                  direction_segment  \\\n",
      "0  No-Bake Nut Cookies  In a heavy 2-quart saucepan, mix brown sugar, ...   \n",
      "1  No-Bake Nut Cookies  Stir over medium heat until mixture bubbles al...   \n",
      "2  No-Bake Nut Cookies                       Boil and stir 5 minutes more   \n",
      "3  No-Bake Nut Cookies                                     Take off heat.   \n",
      "4  No-Bake Nut Cookies              Stir in vanilla and cereal; mix well.   \n",
      "\n",
      "                          tokens_after_lemmatization  \\\n",
      "0  [heavy, 2-quart, saucepan, ,, mix, brown, suga...   \n",
      "1      [stir, medium, heat, mixture, bubble, top, .]   \n",
      "2                            [boil, stir, 5, minute]   \n",
      "3                                    [take, heat, .]   \n",
      "4           [stir, vanilla, cereal, ;, mix, well, .]   \n",
      "\n",
      "                               tokens_after_stemming  \\\n",
      "0  [heavi, 2-quart, saucepan, ,, mix, brown, suga...   \n",
      "1        [stir, medium, heat, mixtur, bubbl, top, .]   \n",
      "2                             [boil, stir, 5, minut]   \n",
      "3                                    [take, heat, .]   \n",
      "4           [stir, vanilla, cereal, ;, mix, well, .]   \n",
      "\n",
      "                                            POS_tags  \n",
      "0  [(heavy, JJ), (2-quart, JJ), (saucepan, NN), (...  \n",
      "1  [(stir, NN), (medium, NN), (heat, NN), (mixtur...  \n",
      "2    [(boil, NN), (stir, NN), (5, CD), (minute, NN)]  \n",
      "3                   [(take, VB), (heat, NN), (., .)]  \n",
      "4  [(stir, NN), (vanilla, NN), (cereal, NN), (;, ...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bleronaidrizi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bleronaidrizi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/bleronaidrizi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/bleronaidrizi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/bleronaidrizi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk import pos_tag\n",
    "import ast\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize the WordNetLemmatizer and PorterStemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Verify the directions column\n",
    "print(\"Sample directions column:\")\n",
    "print(df['directions'].iloc[0])\n",
    "print(type(df['directions'].iloc[0]))\n",
    "\n",
    "# Convert directions to lists if necessary\n",
    "if isinstance(df['directions'][0], str):\n",
    "    try:\n",
    "        df['directions'] = df['directions'].apply(ast.literal_eval)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting directions: {e}\")\n",
    "\n",
    "# Get the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Process directions\n",
    "segmented_tokenized_directions = []\n",
    "for index, row in df.iterrows():\n",
    "    for step in row['directions']:\n",
    "        segments = step.split('. ')\n",
    "        for segment in segments:\n",
    "            clean_segment = segment.strip()\n",
    "            print(f\"Processing segment: '{clean_segment}'\")\n",
    "            if clean_segment:\n",
    "                try:\n",
    "                    tokens = word_tokenize(clean_segment)\n",
    "                    print(f\"Tokens: {tokens}\")\n",
    "\n",
    "                    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "                    lemmatized_tokens = [lemmatizer.lemmatize(word.lower()) for word in filtered_tokens]\n",
    "                    stemmed_tokens = [stemmer.stem(word) for word in lemmatized_tokens]\n",
    "                    pos_tags = pos_tag(lemmatized_tokens)\n",
    "\n",
    "                    segmented_tokenized_directions.append({\n",
    "                        \"title\": row[\"title\"],\n",
    "                        \"direction_segment\": clean_segment,\n",
    "                        \"tokens_after_lemmatization\": lemmatized_tokens,\n",
    "                        \"tokens_after_stemming\": stemmed_tokens,\n",
    "                        \"POS_tags\": pos_tags\n",
    "                    })\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing row {index} segment: '{clean_segment}'. Error: {e}\")\n",
    "\n",
    "# Check if any data was generated\n",
    "if not segmented_tokenized_directions:\n",
    "    raise ValueError(\"No valid data was generated. Please check your input data or processing logic.\")\n",
    "\n",
    "# Convert to DataFrame and export\n",
    "segmented_tokenized_df = pd.DataFrame(segmented_tokenized_directions)\n",
    "segmented_tokenized_df.to_csv(\"../files/6_pos_tagging.csv\", index=False)\n",
    "print(segmented_tokenized_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In natural language processing (NLP), feature extraction is a fundamental task that involves converting raw text data into a format that can be easily processed by machine learning algorithms. There are various techniques available for feature extraction in NLP, each with its own strengths and weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    chicken  chocolate     cream      dish  ingredients   minutes       mix  \\\n",
      "0  0.000000   0.000000  0.000000  0.000000     0.000000  0.418244  0.503853   \n",
      "1  0.707107   0.000000  0.353553  0.353553     0.000000  0.000000  0.353553   \n",
      "2  0.000000   0.000000  0.000000  0.000000     0.707107  0.000000  0.000000   \n",
      "3  0.691269   0.000000  0.230423  0.230423     0.000000  0.191272  0.000000   \n",
      "4  0.000000   0.885664  0.000000  0.000000     0.357274  0.296570  0.000000   \n",
      "\n",
      "       pour      size      stir                  title  \n",
      "0  0.000000  0.000000  0.755780    No-Bake Nut Cookies  \n",
      "1  0.353553  0.000000  0.000000  Jewell Ball'S Chicken  \n",
      "2  0.000000  0.000000  0.707107            Creamy Corn  \n",
      "3  0.230423  0.571207  0.000000          Chicken Funny  \n",
      "4  0.000000  0.000000  0.000000   Reeses Cups(Candy)    \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Kombino të gjitha segmentet e \"directions\" për çdo recetë\n",
    "df['all_directions'] = df['directions'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Përdor TF-IDF për ekstraktim të veçorive\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=10)  # Top 10 fjalë\n",
    "tfidf_matrix = vectorizer.fit_transform(df['all_directions'])\n",
    "\n",
    "# Konverto TF-IDF në DataFrame për lexim të lehtë\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "tfidf_df['title'] = df['title']\n",
    "\n",
    "# Printo rezultatet\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named entity recognition (NER) is a natural language processing (NLP) method that extracts information from text. NER involves detecting and categorizing important information in text known as named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NLP tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Custom NER component\n",
    "@Language.component(\"custom_ner\")\n",
    "def custom_ner(doc):\n",
    "    ingredient_list = nlp.get_pipe(\"custom_ner\").cfg[\"ingredient_list\"]\n",
    "    spans = []\n",
    "    for token in doc:\n",
    "        if token.text.lower() in ingredient_list:\n",
    "            spans.append(Span(doc, token.i, token.i + 1, label=\"INGREDIENT\"))\n",
    "    doc.ents = list(doc.ents) + spans  # Add custom entities to SpaCy's entities\n",
    "    return doc\n",
    "\n",
    "# Convert `ingredients` and `directions` columns to lists\n",
    "for col in ['ingredients', 'directions']:\n",
    "    if isinstance(df[col][0], str):\n",
    "        df[col] = df[col].apply(ast.literal_eval)\n",
    "\n",
    "# Dynamically generate the ingredient list\n",
    "ingredient_list = set()\n",
    "for ingredients in df['ingredients']:\n",
    "    for ingredient in ingredients:\n",
    "        tokens = word_tokenize(ingredient.lower())  # Tokenize each ingredient\n",
    "        filtered_tokens = [word for word in tokens if word.isalpha()]  # Keep only alphabetic words\n",
    "        ingredient_list.update(filtered_tokens)  # Add to the ingredient list\n",
    "\n",
    "ingredient_list = list(ingredient_list)  # Convert to a list\n",
    "\n",
    "print(\"Generated Ingredient List:\", ingredient_list)\n",
    "\n",
    "# Add custom NER to SpaCy pipeline\n",
    "nlp.add_pipe(\"custom_ner\", last=True)\n",
    "nlp.get_pipe(\"custom_ner\").cfg = {\"ingredient_list\": ingredient_list}  # Add ingredient list to the pipe config\n",
    "\n",
    "# NLP Processing and NER Extraction\n",
    "ner_results = []\n",
    "for index, row in df.iterrows():\n",
    "    combined_text = ' '.join(row['directions'])  # Combine all steps into one string\n",
    "    doc = nlp(combined_text)  # Process text using SpaCy\n",
    "    \n",
    "    # Extract entities\n",
    "    ner_list = []\n",
    "    for ent in doc.ents:\n",
    "        ner_list.append(f\"{ent.text} ({ent.label_})\")\n",
    "    \n",
    "    ner_results.append(', '.join(ner_list))\n",
    "\n",
    "# Add the NER results to the dataset\n",
    "df['NLP_NER'] = ner_results\n",
    "\n",
    "# Export to CSV\n",
    "df[['title', 'ingredients', 'NER', 'NLP_NER']].to_csv(\"../files/final_nlp_ner_results.csv\", index=False)\n",
    "\n",
    "# Print the results\n",
    "print(df[['title', 'NLP_NER']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
